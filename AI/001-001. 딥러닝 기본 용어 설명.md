# 001-001. 딥러닝 기본 용어 설명

> Boostcamp Pre-Course
>
> 인공지능 맛보기 - 딥러닝 기본 용어 설명

## What make you a good deep learner?

### Implementation Skills

구현실력. 텐서플로우나 파이토치

머리 속으로 생각하고 아이디어를 가지고 있는 것을 직접 돌려보고 결과를 뽑아보는 것이 중요하다.

### Math Skills

가장 기본이 되는 내용. 수학적인 내용.

여러가지 수학적인 백그라운드가 필요하겠지만, Linear Algebra, Probability가 특히 더 중요하다.

### Knowing a lot of recent Papers

연구를 하는 입장에서 가장 중요한 것은

현재의 트랜드와 최근에 나오는 많은 연구 결과를 아는 것도 중요하다.

앞 부분 수업에서는 딥러닝이 활성화 됐을 떄 논문 중, 기본과 중요한 논문들 위주로 설명할 예정



## Introduction

인공지능 : 사람의 지능을 모방하는 것

분류체계를 잘 잡고 넘어가는 것이 도움이 된다.

인공지능 ⊃ 머신러닝 ⊃ 딥러닝

머신러닝 : 일반적으로는 data-driven approach. 무언가를 학습하고자할 때 데이터를 통해서 한다.

딥러닝 : Neural Networks. 사용하는 모델이 신경망을 사용하는 세부적인 분야가 딥러닝.



딥러닝을 연구한다는 것이 인공지능 전체를 연구하는 것과 동치는 아니다.

Neural Network라는 구조를 활용하면서 데이터를 통해서 무언가를 학습하는 분야에 대해서 알아보자.



**Key Components of Deep Learning**

- The **Data** that the moodel can learn from
  - 자연어를 분류한다고 했을 때 자연어. 말뭉치 등. 비디오를 만들어내기 위해서는 비디오가 데이터.
- The **Model** how to transform the data
  - 학습하고자하는 모델. 강아지와 고양이를 구분한다고 하면 구분하는 라벨
- The **Loss** function that quantifies the badness of the model
  - 모델을 만들고 나면 학습시키기 위한. 추후 설명.
- The **Algorithm** to adjust the parameters to minimize the loss
  - 이런 로스 펑션을 최소화하기 위한 알고리즘

새로운 논문을 보고 새로운 연구를 봤을 때, 이 4가지 항목에 비추어서 연구를 바라보게 되면, 기존 연구에 비해 어떤 장점이 있고 어떤 contribution이 있는지 이해하기가 쉬워진다. 논문 자체를 봤을 때 이 4가지를 주의깊게 봐야한다.

### Data

Data depend on the type of the problem to solve.

풀고자하는 문제에 dependent.

**Classification** : 강아지와 고양이를 구분?

**Semantic Segmentation** : 이미지의 각 픽셀별로 어떤 클래스에 속하는지 확인한다.

​	사진이 주어졌을 때, 이 픽셀은 사람이다. 이 픽셀은 도로다. 이 픽셀은 나무다. 등등 구분

**Detection** : 이미지 안에 물체가 있을 때 이 물체의 bounding box를 찾는 것.

**Pose Estimation** : 이미지 안의 2, 3차원 스켈레톤 정보.

**Visual QnA** : 이미지에 질문이 주어졌을 때 그 답을 구하는 것.

### Model

같은 데이터, 같은 태스크가 주어졌다고 하더라도 모델의 성질에 따라서 좋은 결과가 나올 수도 있고 안좋은 결과가 나올 수도 있다.

결과를 잘 뱉어주기 위한 테크닉들이 들어가게 된다.

AlexNet / GoogleNet / ResNet / DenseNet / LSTM / Deep AutoEncoders / GAN 등의 모델이 있다.

### Loss

모델이 정해져있고 데이터가 정해져있을 때, 이 모델을 어떻게 학습할지

우리는 딥러닝을 다룰거기 때문에 모델은 어떻게든 neural network 형태를 갖게 될 것이고, neural network라는 것은 각 레이어별로 들어가 있는 weight와 bias로 이루어져있음.

그 weight에 각 파라미터들을 어떻게 업데이트 할지, 기준이 되는 loss function을 정의를 하게 된다.

**예시**

일반적으로 regression task 회귀 문제를 푼다고 하면, 뉴럴 네트워크의 출력값과 설계점 사이의 제곱을 최소화 시키는 것이 일반적인 목적이 된다. 그래서 제곱을 평균내서 줄인다 해서 MSE

분류문제 classification task를 풀 때는, 출력값과 라벨데이터의 사이의 크로스 엔트로피라고 하는 loss를 최소화하게 된다.

확률적인 모델을 활용해서 출력값을 단순한 값이 아니라 평균과 분산으로 모델한다고 했을 때 MLE 등..



일반적으로 분류문제를 풀거나 회귀문제를 풀 때, 단순히 loss function이 줄어드는 것을 목적으로 볼 수 있지만 loss function이 줄어든다고 해서 항상 원하는 값이 나온다고는 할 수 없다. 따라서 관계를 잘 이해하는게 새로운 연구를 하는 것에 있어서 중요하다.

데이터에 노이즈가 많다고 해보자. 에러가 되게 클 경우에 MSE 대신에 절대값을 활용해서 한다거나 다른 loss function을 제시하는 것이 중요한 것 처럼, 항상 사용해야 하는 loss function은 없다. 이해 하는 것이 중요

### Optimization Algorithm

최적화 방법은 데이터, 모델, loss function이 정해져있을 때, 네트워크를 어떻게 줄일지에 대한 이야기.

일반적으로 활용하는 방법은 first order method. neural network의 파라미터를 loss function에 대해서 1차 미분한 정보를 활용.

1차 미분한 정보를 그냥 활용하는 것이 SGD. 이것을 변형한 momentum, NAG, Adagrad 등이 들어가게 되고 각각의 특성을 이해하는 것이 중요하다.

loss function을 단순히 무식하게 줄이는게 목적이 아니다. 모델이 학습하지 않은 데이터에서 잘 동작하는 것이 목적이기 때문에 다양한 테크닉을 같이 활용해서 네트워크가 단순히 학습데이터에만 동작하는 것이 아니라 한번도 보지 못한 test data , 실환경에 잘 적용되도록 하는 알고리즘을 배우는 것이 목적



## Historical Review

딥러닝이라고 하는 것이 80, 90s 부터 발전되어왔고, 어떤 방법론들이 큰 임팩트가 있었는지 연구를 보면서 간단히 거칠 것이다.

이런 흐름을 거쳐서 지금의 위상을 갖게 되었구나는 것을 알게 될 것.

2020년 7월 29일에 Denny Britz라는 분이 발표한 논문 "Deep Learning's Most Important Ideas - A Brief Historical Review" 를 토대로 진행

### 2012 - AlexNet

Convolutional Neural Network.

이 네트워크가 하려고 하는 것은 224 X 224 이미지가 들어왔을 때 분류하는 것이 목적.

2012년 ILSVRC (ImageNet Large Scale Visual Recognition Challenge) 대회의 우승.

딥러닝을 이용해서 처음으로 1등을 했던 대회.

모든 기계학습의 판도를 바꿔놓았다.

### 2013 - DQN

이세돌과 알파고의 대결에서 알파고를 만든 딥마인드가 벽돌깨기 인공지능에 사용했던 방법론.

이때까지만 해도 딥마인드는 스타트업.

오늘날의 딥마인드가 있게한 논문.

### 2014 - Encoder / Decoder

NMT (Neural Machine Translation) 문제를 풀기 위함.

풀고자 하는 문제는 다른 언어로 되어있는 문장이 주어졌을 때, 잘 표현해서 우리가 원하는 다른 언어의 단어의 연속으로 뱉어주는 것.

기계어 번역의 트렌드가 바뀌었다.

### 2014 - Adam Optimizer

많은 사람들이 딥러닝을 학습시킨다 했을 때 아담을 많이 사용

아담을 그냥 쓰는 이유는 결과가 잘 나와서.

일반적으로 딥러닝 논문들을 읽다보면, 희한한 방법을 많이 사용한다.

다양한 하이퍼 파라미터 서치를 한다. 어떤 optimizer를 쓸지, base learning rate을 얼마로 할지, learning rate scheduling 은 어떻게 할지 등등. 그 것에 따라서 성능이 많이 달라지게 되는데, 이걸 하려면 많은 컴퓨터 리소스가 필요하다. 예를 들어 구글 같이 1000개의 컨피규레이션을 쓸 수 있다면 괜찮지만, 한번에 두세개 밖에 못해보는 소상공인에게는 아담이라는 방법론은 왠만하면 잘된다는 의미를 갖고 있다.

### 2015 - GAN (Generative Adversarial Network)

이미지를 만들때, 어떻게 만들어낼 수 있을지에 대한 이야기

딥러닝에서 굉장히 중요한 부분.

네트워크가 어떤 generator와 discriminator라는 2개를 만들어서 학습 시키는 것.

일반적으로 논문을 쓰게 되면 마지막 감사의 글에 어떤 리뷰에 감사합니다. 디스커션 감사합니다. 이런 말이 쓰여 있는데, 이 논문에는 Finally we would like to thank Les Trois Brasseurs for stimulating our creativity. 라고 술집에 감사하다는 말이 쓰여있다. 술 먹다가 너무 재미가 없어서 연구를 생각했고 그때 생각난 방법으로 돌아가서 해서 성공 했다는 일화.

### 2015 - Residual Networks (ResNet)

이 연구 덕분에 딥러닝이 딥러닝이 가능해졌다.

왜 딥러닝이냐? 네트워크를 깊게 쌓기 때문이다는 말이 있다.

ResNet 이전에 네트워크를 깊게 쌓으면 테스트를 했을 때 성능이 잘 안나오는, 학습이 잘 안된다는 말이 있었다.

하지만 ResNet이 나오고 나서 트렌드가 바뀌었다.

더 깊게 쌓아도 학습데이터가 아니라 테스트데이터가 잘 나오도록

### 2017 - Transformer

"Attention Is All You Need"

다른 구조에 비해 어떤 장점이 있고 왜 좋은 성능을 낼 수 있는지 추후에 이야기해볼 예정

### 2018 - BERT (fine-tuned NLP models)

앞에서 봤던 transformer 구조를 활용하는데, bidirectional encoder를 활용하는 것.

bert 라는 알고리즘 자체가 중요하긴 하지만

딥러닝의 흐름을 바꾸는데에는 fine-tuned NLP models이 중요한 역할

내가 풀고자 하는 문제가 있을 때 (뉴스기사 작성), 일반적인 문장의 굉장히 다양한 단어들을 가지고 프리트레이닝을 하고 기사에 맞춰서 fine--tuning을 하는 식

### 2019 - BIG Language Models (GPT-X)

약간의 fine-tuning을 통해서 문장, 프로그램, 표 등을 만들 수 있다.

굉장히 많은 파라미터로 되어 있다.

### 2020 - Self Supervised Learning

SimCLR

이 방법론이 하려고 했던 것은, 이미지 분류와 같은 문제를 풀고 싶은데 학습데이터는 한정적일 때

모델, 또는 loss function을 바꿔가면서 좋은 결과를 내는 것이 일반적이었다면

주어진 학습데이터 외에 라벨을 모르는 unsupervise 데이터를 활용하겠다.

라벨은 모르지만 이미지라는 것은 알고 있는 데이터를 학습에 같이 활용하는 방식

이미지를 컴퓨터가 잘 이해할 수 있는 벡터로 잘 바꿀지.

우리가 진짜 풀고자하는 분류문제를 잘 풀기 위함.

학습데이터를 추가로 만들어내서 더 좋은 모델을 만들어내겠다도 가능.



딥러닝의 기본적인 내용을 알아봤고, 2012 ~ 2020까지 어떤 연구들이 중요했는지에 대해서 알아보았다.

